{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DebasmitaKarmakar/AI-Face_recognisation-PETS/blob/main/AI_Face_recognisation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEZjQc4am1Xg"
      },
      "outputs": [],
      "source": [
        "!pip install -q kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buEM9_jKm5Rl"
      },
      "source": [
        "DAY 1\n",
        "\n",
        "IMAGE RECOGNISATION\n",
        "DEEP LEARNING : SUBSET OF ML as in ML IS SUBSET OF AI AND DL IS SUBSET OF ML\n",
        "\n",
        "DATASET : COLLECTION OF DATA\n",
        "\n",
        "PREPROCESSING : FOR EG. BEFORE COOKING U NEED TO CLEAN THE VEGES, SIMILARLY IN ML THING WE NEED TO DO THIS TO GET ANY KIND OF OBSTRUCLES SOLVED\n",
        "\n",
        "EDA (EXPLORATORY DATA ANALYSIS) : UNDERSTANDING WHAT DATA IS BEING PROVIDED"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYJ02wFsNHZd"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KO9P6G8MQQdY"
      },
      "outputs": [],
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKqaYbIqQQf-"
      },
      "outputs": [],
      "source": [
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brjb3Hm3plOh"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"hojjatk/mnist-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XlqrA71nplRL"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WjcTptX0fdAv"
      },
      "outputs": [],
      "source": [
        "#for preprocessing we need to reshape and normalise the data\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "X_train = X_train.reshape(-1,28,28,1)\n",
        "X_test = X_test.reshape(-1,28,28,1)\n",
        "print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAezm-_MplUk"
      },
      "outputs": [],
      "source": [
        "#EDA TECHNIQUE\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(8,8))\n",
        "for i in range(9):\n",
        "  plt.subplot(3,3,i+1)\n",
        "  plt.imshow(X_train[i].reshape(28,28),cmap='gray')\n",
        "  plt.title(f\"Label: {y_train[i]}\")\n",
        "  plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFWZUU2rQQii"
      },
      "outputs": [],
      "source": [
        "#EDA TECHNIQUE\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10,10))\n",
        "for i in range(9):\n",
        "  plt.subplot(3,3,i+1)\n",
        "  plt.imshow(X_train[i].reshape(28,28),cmap='gray')\n",
        "  plt.title(f\"Label: {y_train[i]}\")\n",
        "  plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8_x8h_RQQlD"
      },
      "outputs": [],
      "source": [
        "#train our datasets of how its going to behave\n",
        "(X_train_c10, y_train_c10), (X_test_c10, y_test_c10) = tf.keras.datasets.cifar10.load_data()\n",
        "X_train_c10 = X_train_c10 / 255.0\n",
        "X_test_c10 = X_test_c10 / 255.0\n",
        "print(f\"CIFAR-10 Train shape: {X_test_c10.shape}\")\n",
        "#load dataset and normalize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0yB_w4zeJzQ"
      },
      "source": [
        "DAY 2\n",
        "\n",
        " CNN MODEL- USED TO WORK AND ADAPT LEARNING PATTERNS WITH IMAGES\n",
        "\n",
        " MODEL TRAINING -\n",
        "\n",
        " WHY DROPOUTS ARE USED - ITS A TRICK TO MAKE THE MODEL MORE FLEXIBLE BY PREVENTING IT FROM MEMORISING THE TRAINING DATA\n",
        "\n",
        " HOW TO EVALUATE MODELS -"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_r48OhLstjKe"
      },
      "outputs": [],
      "source": [
        "#MAKING CNN MODEL WITH KERAS TO RECOGNISE DIGITS FROM THE DATASET\n",
        "from tensorflow.keras.models import Sequential  #seq means layers will be added one after another\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "#conv2d detects the pattern and features, maxpooling makes image smaller and focus on main part, flatten converts 2d to 1d, dense will see the layers and make proper prediction\n",
        "model = Sequential([\n",
        "    #first cnn layer\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)), #input shape shows pixel range in gray scale\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Dropout(0.25),\n",
        "    #second cnn layer\n",
        "    Conv2D(64, (3, 3), activation='relu'), #activation relu will only keep the necesaary informations i.e keep the positive values\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Dropout(0.25),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    #final layer\n",
        "    Dense(10, activation='softmax') # 10 for 10 output classes, softmax is used to turn outputs into probabilities\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "#adam is a smart algo to add weights on the dataset , sparse one is used to label a single integer i.e single digit markdown\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTiyB7XmtjMR"
      },
      "outputs": [],
      "source": [
        "#training dataset\n",
        "history = model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.2)\n",
        "#epoch used as the entire training process will keep on revarifying atleast 5 times, split is on 0.2 i.e 20%\n",
        "#like 20% of dataset will be kept as validation data to check how model is learning without touching the training data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sXbqjUBtjPo"
      },
      "outputs": [],
      "source": [
        "#we will plot a graph to show how well the model is got trained and how well it shows up to the validation data after each round\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['accuracy'], label='Train')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBHYRTAjl7qv"
      },
      "outputs": [],
      "source": [
        "test_loss,test_acc=model.evaluate(X_test,y_test)\n",
        "print(\"Test Accuracy:{test_acc:.4f}\")\n",
        "y_pred=model.predict(X_test).argmax(axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YTNTaaF4WbU"
      },
      "source": [
        "DAY 3\n",
        "\n",
        "what is deeper cnn ? - A convolutional neural network with more layers to learn complex features, used in advanced image recognition. it has more layers to help learn in more detailed way and analys more complex images. first was detecting textures, 2nd layer we detect shapes and paths, deeper layer gives the object detection and the category that the object belongs.\n",
        "\n",
        "what is data augmentation - Creating more training data by modifying existing samples (flip, rotate, crop, add noise, etc.) to improve model generalization.\n",
        "\n",
        "what is advance evolution matrices ? - Performance measures beyond accuracy, like precision, recall, F1-score, ROC-AUC, IoU, for better model assessment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSXej8lXl7tW"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "#ImageDataGenerator is a tool used to create more training images by adding random changes to the existing datasets\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=15, #rotate image by 15 degree\n",
        "    width_shift_range=0.1, #10% of the original width being shifted\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        ")\n",
        "datagen.fit(X_train_c10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pelLpHFl7wo"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "#BatchNormalization will adjust all image and balance then like resetting\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
        "    Dropout(0.25),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Dropout(0.25),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Dropout(0.25),\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(datagen.flow(X_train_c10,y_train_c10,batch_size=32),epochs=10,validation_data=(X_test_c10,y_test_c10))\n",
        "\n",
        "#making more advanced cnn like for coloured images too, it learns step by step like first simple shapes and then details\n",
        "#activation to learn faster and uses dropout to prevent overfitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGecDNfuQQod"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "import numpy as np\n",
        "\n",
        "y_pred_c10=model.predict(X_test_c10).argmax(axis=1)\n",
        "cm=confusion_matrix(y_test_c10,y_pred_c10)\n",
        "print(classification_report(y_test_c10,y_pred_c10))\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cm,annot=True,fmt='d')  #density most will give darkest part\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.show()\n",
        "\n",
        "#this not only gives result but also shows where the model made mistakes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJOSLR63FOO4"
      },
      "source": [
        "DAY 4\n",
        "\n",
        "WHAT IS PRE TRAINED MODEL ? - A model already trained on a big dataset (like ImageNet or Wikipedia). You can use it as a starting point instead of training from scratch. IT is a form of neural network that has already been trained in larger datasets. it has already learned to detect image,shape,colour etc.\n",
        "\n",
        "WHAT IS TRANSFER LEARNING ? -  Taking a pre-trained model and adapting it to a new, related problem. Keeps the general features it learned, changes the last layers for your task. we dont train the model from 0 but we transfer the related problems.\n",
        "\n",
        "HOW TO FINE-TUNE AND OPTIMIZE ? - First freeze most layers and train the new ones on your data, then unfreeze some layers and train with a low learning rate. Use tricks like data augmentation, dropout, and learning rate schedules to improve results. Finetune is abt unfreezing all the layers and then retaining then small learning data so that we can optimize it better.\n",
        "\n",
        "HOW TO DEPLOY A MODEL ? - Save the trained model, put it on a server, cloud, or mobile, and create an interface (like an API or app) so people can use it for predictions. will use streamlit. Deployment is the stage where you take your trained model and make it accessible for real use. First, you save the model using a format like .h5 in Keras or .pkl/.joblib in scikit-learn. For creating a user interface, tools like Streamlit allow you to quickly build an interactive web app where users can upload data and get predictions in real time. If you need more control or production-level APIs, frameworks like Flask or FastAPI are popular choices. For larger scale or enterprise deployment, cloud platforms such as AWS SageMaker, Google Vertex AI, or Azure ML provide hosting, scaling, and monitoring features. Models can also be optimized for mobile using TensorFlow Lite for Android or Core ML for iOS, and for small devices like Raspberry Pi using ONNX Runtime or TFLite. Finally, you’ll need a hosting solution—such as Heroku, Render, or AWS EC2—to make your model available online, often through an API so that other applications can send data and receive predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-w7Z-y87JJk2"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets download -d tongpython/cat-and-dog  #pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9tzzMJdJJnZ"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile('/content/cat-and-dog.zip','r')as zip_ref:\n",
        "    zip_ref.extractall(r\"/content/PETS-Dogs_Cats\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3mVPLDdJJq0"
      },
      "outputs": [],
      "source": [
        "#PREPARE OR PREPROCESS THIS NEW PRETRAINED DATASET\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    validation_split = 0.2,\n",
        ")\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    '/content/PETS-Dogs_Cats',\n",
        "    target_size=(224,224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    subset='training'\n",
        ")\n",
        "val_generator = train_datagen.flow_from_directory(\n",
        "    '/content/PETS-Dogs_Cats',\n",
        "    target_size=(224,224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "#we told python to go into the cats and dogs dataset and resize the images into 224 x 224 and normalise to get pixels from 0 to 1\n",
        "#and further we will keep 80% data for training and 20% for testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvUjoBOOJJ0i"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
        "\n",
        "base_model=MobileNetV2(weights='imagenet',include_top=False,input_shape=(224,224,3))\n",
        "base_model.trainable=False\n",
        "\n",
        "model=Sequential([base_model,GlobalAveragePooling2D(),Dense(128,activation='relu'),Dense(1,activation='sigmoid')])\n",
        "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
        "model.fit(train_generator,epochs=5,validation_data=val_generator)\n",
        "\n",
        "#we borowed the brain mobilenet v2 that already knows how to look at pictures and froze it so it doesnt forget and then we added out own decision making layer to train only the new part to assign if it is a cat or dog"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ltu3Ao6xPoCr"
      },
      "outputs": [],
      "source": [
        "base_model.trainable = True\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(train_generator, epochs=3, validation_data=val_generator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faLOKV3lPoFI"
      },
      "outputs": [],
      "source": [
        "# Save the trained model\n",
        "model.save('mobilenet_cats_dogs.h5')\n",
        "#To load\n",
        "import tensorflow as tf\n",
        "loaded_model=tf.keras.models.load_model('mobilenet_cats_dogs.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvawP0PvPoHg"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve, auc   #roc calculates the point and auc is area under the cover\n",
        "y_pred_proba = loaded_model.predict(val_generator)\n",
        "\"\"\"\n",
        "uses a loaded model to predict prob  of each image in validation set\n",
        "since its binary classification, the value near 1 is more likely a dog and near 0 is cat more likely\n",
        "\"\"\"\n",
        "fpr, tpr, _ = roc_curve(val_generator.classes, y_pred_proba)\n",
        "roc_auc = auc(fpr, tpr)    # to see how well it diff btwn cat and dog\n",
        "\"\"\"\n",
        "calculates area under the roc curve, for 1 it is perfect model and for 0.5 it is random guessing\n",
        "\"\"\"\n",
        "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "#roc curve sees ur model from every posible variants"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Number of images to show\n",
        "num_images = 12\n",
        "indices = np.random.choice(len(X_train), num_images, replace=False)\n",
        "\n",
        "# Plot the images\n",
        "plt.figure(figsize=(6,6))\n",
        "for i, idx in enumerate(indices):\n",
        "    plt.subplot(3, 4, i + 1)  # 3 rows, 4 columns\n",
        "    plt.imshow(X_train[idx].astype(\"uint8\"))  # or remove .astype if not needed\n",
        "    plt.title(f\"Label: {y_train[idx]}\")\n",
        "    plt.axis(\"off\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1Jl6r7i8h9ty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Example values — replace these with your actual results\n",
        "methods = [\"Baseline CNN\", \"Augmented CNN\", \"TL (Frozen)\", \"TL (Fine-tuned)\"]\n",
        "accuracy = [0.85, 0.88, 0.92, 0.95]  # Replace with your real accuracies\n",
        "\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.bar(methods, accuracy, color=[\"skyblue\", \"orange\", \"green\", \"red\"])\n",
        "plt.ylim(0, 1)  # since accuracy is between 0 and 1\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Model Accuracy Comparison\")\n",
        "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8WjOjFmdh3H0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zs9KStmHE0mp"
      },
      "source": [
        "DAY 5\n",
        "\n",
        "how to predict on new data ? - Load the saved model, preprocess the new input to match the model’s expected shape and scaling, and use model.predict() to get results, applying thresholds or argmax() as needed.\n",
        "\n",
        "how to create portfolio assets?-Collect and prepare visuals like training curves, confusion matrices, model diagrams, and sample predictions, along with clean code snippets for clarity.\n",
        "\n",
        "how to present results ?- Clearly outline the problem statement, dataset, model details, performance metrics, and visual examples, followed by insights, limitations, and suggestions for improvement.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3KN5pYFPoJ_"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "import numpy as np\n",
        "\n",
        "img_path = 'cat.jpg' # Replace with your filename\n",
        "img = image.load_img(img_path, target_size=(224, 224))\n",
        "img_array = image.img_to_array(img)\n",
        "img_array = np.expand_dims(img_array, axis=0)\n",
        "img_array = preprocess_input(img_array)\n",
        "\n",
        "class_names = {0: \"Cat\", 1: \"Dog\"}\n",
        "pred = loaded_model.predict(img_array)\n",
        "predicted_class_index = np.argmax(pred, axis=1)[0] if pred.shape[1] > 1 else int(pred[0][0] > 0.5)\n",
        "print(\"Predicted class:\", class_names[predicted_class_index])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pJc_B5fPoNV"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "datasets = ['MNIST', 'CIFAR-10', 'Cats vs. Dogs']\n",
        "accuracies = [0.98, 0.75, 0.84]\n",
        "plt.bar(datasets, accuracies, color=['#36A2EB', '#FF6384', '#4BC0C0'])\n",
        "plt.title('Model Accuracies Across Datasets')\n",
        "plt.xlabel('Dataset')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim(0,1)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install streamlit tensorflow opencv-python pillow\n"
      ],
      "metadata": {
        "id": "Mz0xggk-pEeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit pyngrok"
      ],
      "metadata": {
        "id": "2vZTC4hQK-g5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# -----------------------------\n",
        "# Load trained face recognition model\n",
        "# -----------------------------\n",
        "@st.cache_resource\n",
        "def load_model():\n",
        "    return tf.keras.models.load_model(\"mobilenet_cats_dogs.h5\")\n",
        "\n",
        "model = load_model()\n",
        "\n",
        "# Class name mapping (change these names to match your dataset)\n",
        "class_names = {0: \"Cat\", 1: \"Dog\"}\n",
        "\n",
        "# -----------------------------\n",
        "# Page config & title\n",
        "# -----------------------------\n",
        "st.set_page_config(\n",
        "    page_title=\"FaceVision: AI Recognition\",\n",
        "    page_icon=\"🧠\",\n",
        "    layout=\"centered\"\n",
        ")\n",
        "\n",
        "st.markdown(\n",
        "    \"<h1 style='text-align: center; color: #4CAF50;'>🧠 FaceVision: AI Recognition</h1>\",\n",
        "    unsafe_allow_html=True\n",
        ")\n",
        "st.markdown(\n",
        "    \"<p style='text-align: center; font-size: 16px;'>Upload an image and let our AI model identify it with high precision.</p>\",\n",
        "    unsafe_allow_html=True\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# Sidebar input\n",
        "# -----------------------------\n",
        "st.sidebar.header(\"📷 Upload Single Image\")\n",
        "uploaded_file = st.sidebar.file_uploader(\n",
        "    \"Choose an image...\",\n",
        "    type=[\"jpg\", \"jpeg\", \"png\"]\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# Preprocessing function\n",
        "# -----------------------------\n",
        "def preprocess_image(image, target_size=(224, 224)):\n",
        "    img = np.array(image)\n",
        "    if img.shape[-1] == 4:  # remove alpha channel if RGBA\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_RGBA2RGB)\n",
        "    img = cv2.resize(img, target_size)\n",
        "    img = img / 255.0\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    return img\n",
        "\n",
        "# -----------------------------\n",
        "# Single image prediction\n",
        "# -----------------------------\n",
        "if uploaded_file:\n",
        "    image = Image.open(uploaded_file)\n",
        "    st.image(image, caption=\"📸 Uploaded Image\", use_column_width=True)\n",
        "\n",
        "    if st.button(\"🔍 Predict Identity\"):\n",
        "        with st.spinner(\"🤖 Analyzing image...\"):\n",
        "            processed_img = preprocess_image(image)\n",
        "            predictions = model.predict(processed_img)\n",
        "            predicted_class_index = np.argmax(predictions, axis=1)[0]\n",
        "            predicted_label = class_names.get(predicted_class_index, str(predicted_class_index))\n",
        "            confidence = float(np.max(predictions) * 100)\n",
        "\n",
        "        # Stylish Result Card\n",
        "        st.markdown(\n",
        "            f\"\"\"\n",
        "            <div style=\"background-color: #2E7D32; padding: 15px; border-radius: 10px; margin-top: 10px;\">\n",
        "                <h3 style=\"color: white; text-align: center;\">Predicted Class: {predicted_label}</h3>\n",
        "            </div>\n",
        "            \"\"\",\n",
        "            unsafe_allow_html=True\n",
        "        )\n",
        "\n",
        "        st.markdown(\n",
        "            f\"\"\"\n",
        "            <div style=\"background-color: #1565C0; padding: 15px; border-radius: 10px; margin-top: 10px;\">\n",
        "                <h4 style=\"color: white; text-align: center;\">Confidence: {confidence:.2f}%</h4>\n",
        "            </div>\n",
        "            \"\"\",\n",
        "            unsafe_allow_html=True\n",
        "        )\n",
        "\n",
        "# -----------------------------\n",
        "# Batch prediction section\n",
        "# -----------------------------\n",
        "st.markdown(\"---\")\n",
        "st.markdown(\"### 📂 Batch Prediction for Multiple Images\")\n",
        "batch_file = st.file_uploader(\n",
        "    \"Upload a ZIP file containing multiple images\",\n",
        "    type=\"zip\"\n",
        ")\n",
        "\n",
        "if batch_file is not None:\n",
        "    with zipfile.ZipFile(batch_file, \"r\") as zip_ref:\n",
        "        zip_ref.extractall(\"batch_images\")\n",
        "\n",
        "    results = []\n",
        "    for img_name in os.listdir(\"batch_images\"):\n",
        "        img_path = os.path.join(\"batch_images\", img_name)\n",
        "        image = Image.open(img_path)\n",
        "        processed_img = preprocess_image(image)\n",
        "        predictions = model.predict(processed_img)\n",
        "        predicted_class_index = np.argmax(predictions, axis=1)[0]\n",
        "        predicted_label = class_names.get(predicted_class_index, str(predicted_class_index))\n",
        "        confidence = float(np.max(predictions) * 100)\n",
        "        results.append([img_name, predicted_label, confidence])\n",
        "\n",
        "    results_df = pd.DataFrame(results, columns=[\"Image Name\", \"Predicted Class\", \"Confidence (%)\"])\n",
        "    st.dataframe(results_df)\n",
        "\n",
        "    csv = results_df.to_csv(index=False).encode('utf-8')\n",
        "    st.download_button(\n",
        "        \"📥 Download Batch Predictions CSV\",\n",
        "        csv,\n",
        "        file_name='face_predictions.csv',\n",
        "        mime='text/csv'\n",
        "    )\n",
        "\n",
        "st.markdown(\"---\")\n",
        "st.markdown(\"<p style='text-align: center;'>👩‍💻 <i>Built with ❤️ for AI and Computer Vision enthusiasts.</i></p>\", unsafe_allow_html=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "rMXIrzYzpEgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken 309KFcPtopLpMIdnfjFPw7XLBsW_67HT4VY7gGeeYZ6Yhd5nu"
      ],
      "metadata": {
        "id": "qUatIP0gpEjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import threading\n",
        "\n",
        "def run_streamlit():\n",
        "    os.system('streamlit run app.py --server.port 8501')\n",
        "\n",
        "thread = threading.Thread(target = run_streamlit)\n",
        "thread.start()"
      ],
      "metadata": {
        "id": "ja9pec5vpElb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import time\n",
        "\n",
        "#wait a few sec\n",
        "time.sleep(5)\n",
        "\n",
        "#create a tunnel to streamlit port 8501\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"Your Streamlit app is live here:\", public_url)"
      ],
      "metadata": {
        "id": "aUle7nThpEoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py"
      ],
      "metadata": {
        "id": "4xcv5kPfpEqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_generator.class_indices)\n"
      ],
      "metadata": {
        "id": "xwcDjgeGpEsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git init\n",
        "!git config --global user.email \"you@example.com\"\n",
        "!git config --global user.name \"Your Name\"\n",
        "!git add .\n",
        "!git commit -m \"Initial commit - FaceVision AI\"\n",
        "!git branch -M main\n",
        "!git remote add origin https://github.com/YourUsername/FaceVision-AI.git\n",
        "!git push -u origin main\n"
      ],
      "metadata": {
        "id": "tiAUxE1NpEvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r FaceVision.zip app.py mobilenet_cats_dogs.h5 requirements.txt README.md sample_images\n"
      ],
      "metadata": {
        "id": "C3D5LukftvS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('FaceVision.zip')\n"
      ],
      "metadata": {
        "id": "efqXTKQFt-Sd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyNMB1xfOZCuQPAPwWlmrzr1",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}